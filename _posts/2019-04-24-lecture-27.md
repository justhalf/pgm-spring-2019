---
layout: distill
title: "Lecture 27: Scalable Algorithms and Systems for Learning, Inference, and Prediction"
description: Notes for Lecture 27
date: 2019-04-24

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Aldrian Obaja Muis  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Tanmaya Shekhar Dabral
    url: "#"
  - name: Jueheng Zhu
    url: "#"

editors:
  - name: Lisa Lee  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  Scalable algorithms and systems for learning, inference, and prediction
---

## Preliminaries

### Two major challenges in real-world machine learning:

- **Massive datasets:** For example, Wikipedia, YouTube videos, Tweets etc.
- **Massive models:** Parameters in billions for complicated architectures like deep learning models, Bayesian architectures etc. Even simple models like regression can have a large number of parameters.

We need scalable ML algorithms to overcome these challenges. We wish to develop distributed ML algorithms whose performance scales almost linearly with the number of machines.

We can coarsely divide the machine learning methods into two types:

- **Probabilistic programs:** Markov Chain Monte Carlo methods, Variational Inference etc.
- **Optimization programs:** Like gradient descent and related algorithms

We see that ML algorithms have a property of being iterative-convergent. That is incremental updates are made to the state of the model until  the state is stationary. Another important property of such algorithms is the fact that unlike most traditional algorithms, where each step must be perfect to get the final output, since ML algorithms are essentially a search for a state over a space, slight noise in the trajectories are well-tolerated. That is, these **ML algorithms can self-heal**. These are important properties that facilitate distributed machine learning. We can now formally define an ML program as a search for a $\theta^*$ such that:

$$
\theta^* = \arg\max_{\theta}\mathcal{L}(\{x_i, y_i\}_{i=1}^N;\theta) + \Omega(\theta)
$$

where $\Omega$ is some regularization term and $\mathcal{L}$ is some function of the data and the parameters. A typical algorithm to find this $\theta^*$ looks like this:

~~~
for t from 1 to T:
  doSomething();
  updateParameters();
  doSomethingElse();
~~~

Where the `updateParameters()` function looks something like:

$$
\theta^{t+1} = g(\theta^t, \Delta_f\theta(\mathcal{D}))
$$

where $\Delta_f\theta(\mathcal{D})$ is an incremental update calculated as a function of the dataset and the previous value of the parameters. Since function scales with both the data and the parameters, it is typically the most computationally expensive portion of the entire algorithm and benefits from parallelization.

We can broadly categorize the parallelization techniques that we use into two categories:

- **Data parallel methods:** The data into smaller chunks and sent to different machines, which calculate updates relevant to a particular chunk, after which some sort of an aggregation step is performed to combine the work done by different machines.
- **Model parallel methods:** The model parameters are divided into different sets and sent to different machines and each machine calculates the updates for those specific parameters.

Of course, a combination of these two approaches is also possible.

## Parallelization for Optimization Algorithms

### A concrete optimization problem:

Consider the case of a simple linear regression with a squared error loss. We wish to find:

$$
\beta^* = \arg\min_\beta \frac{1}{2}\lVert Y - \beta X\rVert_2^2 + \lambda\Omega(\beta)
$$

Where $\Omega$ is a regularization term which may be used to induce sparsity, or incorporate domain knowledge. The form of $\Omega$ may vary depending on what the underlying priors are about the structure of the problem. This tells us that linear regression is actually a collection of models, where we can pick a data-fitting loss function and a regularization term and take a weighted sum of the two. Therefore, any parallel algorithms that one might come up with should be fairly general.

### Recap: The sequential Stochastic Gradient Descent (SGD)
  Consider the following optimization problem:

  $$
  \theta^* = \arg\min_\theta \mathbb{E}_{x}[\mathcal{L(x;\theta)}]
  $$

  Vanilla gradient descent updates the parameters in the following manner:

  $$
  \theta^{t+1} = \theta^{t} - \gamma\frac{1}{n}\sum_{i=1}^{n}\nabla_{\theta^t}\mathcal{L(x_i;\theta^t)}
  $$

  Stochastic gradient descent uses a noisy estimate of the gradient using a single point:

  $$
  \theta^{t+1} = \theta^{t} - \gamma\nabla_{\theta^t}\mathcal{L(x_i;\theta^t)}
  $$

  This converges almost surely to the global optimum for convex problems. In practice, we actually use a batch of data points instead of a single one, which decreases the variance of the gradient and also allows us to use efficient matrix multiplication libraries.

### Parallel Stochastic Gradient Descent (PSGD) [Zinkevich et al., 2010]:

- A data-parallel extension to the vanilla stochastic gradient descent, wherein you divide the dataset into chunks and send a chunk to one machine with a copy of the parameters.
- Each machine runs SGD with its chunk of data till it converges.
- Finally the different versions of the parameters are aggregated, for example, by taking the mean of the parameters.
- **Downside:** Taking the mean of parameter copies that have individually converged to minimas need not necessarily mean that the resultant parameters lie in a minima.

### Hogwild!: A lock-free approach to PSGD [Rechtet al., 2011]

Our goal is to optimize a function of the form, which is common in sparse models:

$$
f(\theta) = \sum_{e\in E}f_e(\theta_e)
$$

where $E$ is the set of indices of all the parameters, $e$ is a subset of these indices, $f_e$ is a function that operates only on the parameters which have their index in $e$ and $\theta_e$ is the set of parameters with their index in $e$. In such a case, we can use the Hogwild! algorithm as follows:
~~~
for each core in parallel:
  for i from 1 to T:
    Sample e uniformly at random from E
    Read current parameter set θ_e and calculate gradients for f_e
    Sample a single coordinate from θ_e
    Perform SGD on that single coordinate with a small step size
~~~

We can ignore memory-locking the parameters when computing the updates because the chances of collisions are very low. 

This algorithm works very well for single machines with multiple processing units, but is less ideal for distributed systems due to the inherent delay in the communication between nodes. In particular, in case of uncontrolled delays in the communication between machines in lock-free environments, we face two issues:

- A high variance in the delay, which is typical of distributed systems, causes a slower convergence
- A higher delay increases the variance of the parameters, that is, causes instability during convergence 

### Parallel SGD with Key-Value
In this paradigm, we maintain a parameter server whose sole job is to maintain a master copy of the parameters and handle synchronization among the updates from the worker nodes, whose sole job is to calculate parameter updates and send to the server. There can be different synchronization schemes in place:

- **Bulk synchronous parallel:** In this scheme, all the worker nodes get a fresh copy of the parameters from the master, calculate the updates using these parameters and send the updates to the master. The master then waits till it has received an update from all the nodes and then updates the parameters. It then broadcasts the new parameters to all the nodes. This is slowed down by the fact that the nodes have to wait for all other nodes to finish to receive a new copy.
- **Asynchronous parallel:** Essentially Hogwild!. The worker nodes do their jobs without waiting for the slower nodes to catch up. 
- **Stale synchronous/bounded asynchronous parallel:** A middle ground between bulk synchronous and asynchronous schemes. Unlike the bulk synchronous scheme, the worker nodes do not wait for all the other nodes to have calculated an update, but unlike the asynchronous scheme, they're not allowed to get too far ahead of the slower nodes either. In particular, there is a limit (staleness threshold) to the number of steps a particular worker can be ahead by, after which it must wait for the slower threads to catch up.

Empirically, it has been shown that stale synchronous parallel updates converge faster (in seconds) than the other two.

### Sequential Coordinate Descent

Another popular optimization technique that is commonly used is the Coordinate Descent Algorithm. It works as follows:

- Randomly pick a single parameter $\theta_i$ from the vector of parameters $\theta$
- Given the loss function $\mathcal{L}(\theta)$, solve the equation $\frac{\partial \mathcal{L}(\theta)}{\partial\theta_i} = 0$ and update the parameter $\theta_i$ to the solution
- Repeat

This algorithm suits Lasso regression very well as the partial derivative turns out to be a linear function of $\theta_i$, and therefore the equation is easily solvable.

#### Block Coordinate Descent:

Coordinate Descent lends itself to a simple extension called Block Coordinate Descent. Instead of picking a single parameter in the first step, we select a subset of parameters. In the second step, we solve a system of equations by obtained by setting the partial derivatives with respect to all the parameters in the subset to zero.

### Shotgun, a Parallel Coordinate Descent Algorithm [Bradley et al. 2011]

- An algorithm very similar to Hogwild!, in that it is simple to implement.
- Each worker randomly picks a coordinate (a single parameter) to update.
- The updates are done in parallel with no locks.
- If the features are nearly independent, it scales almost linearly with the number of machines.
- As can be seen in the image below, oncurrent updates to coordinates often overshoot if the features are correlated and may slow down the covergence, or in the worst case, cause a divergence.

<img src="{{ '/assets/img/notes/lecture-27/pcsd.png' | relative_url }}" />

### Block-greedy Coordinate Descent [Scherrer et al., 2012]

- A generalization of the othe paralllel coordinate descent algorithms.
- The coordinates are divided into $B$ blocks such that the inter-block correlation is minimal.
- Each node picks a different block, and then picks a parameter from that block to update.
- This algorithm has a sublinear convergence.
- Dividing the parameters into uncorrelated blocks is difficult.

### Parallel Coordinate Descent with Dynamic Scheduler [Lee et al., 2014]

- STRADS (STRucture-Aware Dynamic Scheduler) is a scheduler that allows dynamic scheduling of parameter updates in coordinate descent.
- It focuses on two things:
  - Dependency checking: If two coordinates are correlated, do not compute them in parallel.
  - Prioritization: Choose the coordinate to update with a probability proportional to its historic rate of change. Intuitively, a parameter that changes rapidly will probably cause a quicker decrease of the loss function.

These two things together can provide an order of magnitude increase in the rate of convergence.

### Advanced Optimization for Complex Constraints: Proximal Gradient methods:

#### Projected Gradients:

Suppose we have an optimization problem:

$$
\min_w f(w) \text{ such that } w \in C
$$

where $f$ is some smooth function and C is a convex set of points. We can define a $0-\infty$ indicator function $\iota_C$ such that:

$$
\iota_C(w) = \begin{cases}0 & w \in C\\ \infty & w \not\in C\end{cases}
$$

The optimization then reduces to:

$$
\min_w f(w) + \iota_C(w)
$$

An iterative solution can be obtained using the projected gradient descent algorithm. One iteration looks like:

- $v \leftarrow w - \eta\nabla f(w)$
- $w \leftarrow \arg\min_z \frac{1}{2\eta}\lVert v - z\rVert_2^2 + \iota_C(v) = \arg\min_{z \in C}\frac{1}{2\eta}\lVert v - z_2\rVert_2^2$

Intuitively, it is equivalent to taking an unconstrained step in the direction of the gradient, and then projecting the new position to the closest point in the feasible set.

#### Proximal Gradients:

Proximal gradients are a generalization of projected gradients, in which the indicator function $\iota_C$ is replaced by an arbitrary function $g(v)$, which may model constraints, regularizations etc. Intuitively, now, not only can we restrict $w$ to a feasible region, but can also say which regions are better than the others. Now one iteration of the algorithm looks like:

- $v \leftarrow w - \eta\nabla f(w)$
- $w \leftarrow \arg\min_z \frac{1}{2\eta}\lVert v - z\rVert_2^2 + g(v)$

The minimization problem in the second step has been well-studied, and closed form operators have been derived for the expression $\arg\min_z \frac{1}{2\eta}\lVert v - z\rVert_2^2 + g(v)$ for different $g$. We denote:

$$
P^\eta_g(v) = \arg\min_z \frac{1}{2\eta}\lVert v - z\rVert_2^2 + g(v)
$$

Also, for certain conditions derived by [Yu, 2013], $P^\eta_{g_1+g_2}(w) = p^\eta_{g_1}(p^\eta_{g_2}(w))$, allowing us to model combinations of constraints/regularizations. 

#### Accelerated Proximal Gradients:

- PG convergence rate is $O(1/\eta t)$
- If we add a momentum term as follows, it improves to $O(1/\eta t^2)$
- It is done as follows:
  - $v^t \leftarrow w^t - \eta\nabla f(w^t)$
  - $u^t \leftarrow P^\eta_g(v^t)$
  - $w^{t+1} \leftarrow u^t + \frac{t-1}{t+1}(u^t - u^{t-1})$

This is similar to a "momentum" term often added to a standard gradient descent.

### Parallel Accelerated Proximal Gradient Descent

Parallel (accelerated) PG is typically implemented using a parameter server in a bulk-synchronous fashion. The typical strategy goes as follows:
- Gradients for the first step are computed on the workers, since this is the step that scales with the data.
- The gradients are aggregated on the servers and the proximal operators are applied
- The momentum term is calculated and the parameters updated.
- The server then broadcasts the new parameters.

For empirical speedup, one can also apply Hogwild! style asynchronous updates for non-accelerated PG. However, the theoretical and empirical behavior of accelerated PG under asynchrony is an open question.

## MCMC algorithms

Slide 51 skipped

Slide 52:

- We want to reach convergence faster, but naively having multiple chains is not doing this, it just gives us more samples after convergence.

Slide 54-56 (solution 1: aux var)

- Re-parameterization to make sampling amenable to parallelization
- Introduce aux var (AV) $$\psi$$
- (Aldrian's comment: similar to homework)

Slides 57-59 (solution 2: subposterior)

- Like multiple chains in Slide 52, but with subset of data
- How to combine? Naive averaging (green circle) is wrong. Use special way to recombine (slide 58)

Slides 60-68 (solution 3: parallel Gibbs sampling)

- $$B$$ is word-topic matrix, $$\delta$$ is (sufficient statistics of) topic vectors
- Split delta, copy B, since it is robust to small change, update, copy back (slide 64-67)
- Problem: no longer guaranteed to converge
- Bigger problem: communication time could be longer than computation

Slides 70-72 (model parallel: GraphLab)

- Do vertex cut, create graph, divide
- Async, but vertex cut takes non-trival time

Slides 73-79 (LightLDA: improvement of GraphLab)

- Split not using vertex cut, but simply divide the matrix

Slide 80: Summary of Distributed ML Algorithms

## Distributed Systems for ML

- Previously was more mathematical, how about the actual real-world software?

Slide 83-84:

- Problems: Hotter machines, vibrating machines, network used by others, other users using computations, etc.
- Need to consider all these aspects

Slides 86-87 skipped (Spark fixes HDFS by using memory, but not the Bulk Sync)

**Slide 90: Keypoint of this lecture**

Slide 91-92: more explanation on Slide 90

Slide 93

- Data parallelism is easy
- Model parallelism is not, naive way usually does not work
- Key point: need *schedule*

Slide 94 (Intrinsic properties of ML programs):

- Error tolerance
- Dynamic structural dependency
- Non-uniform convergence
- Summary: unlike traditional programs, here we have some tolerance

Slides 95-97 (middle-ground between bulk and async: stale sync)

Slides 100-109 skipped (covered before)

Slide 110 (understanding the convergence)

Slide 113 (bounds)

- Expectation
- Probabilistic
- Variance

Slide 117 (intuition and sequential)

Slide 119

- No need to delve too deep, just understand what is the influence of each factors

Slide 131

- Summary
